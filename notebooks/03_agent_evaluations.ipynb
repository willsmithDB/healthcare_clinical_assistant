{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e84ff04b-9283-4fe6-ba4b-d492295ce44a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qqqq -U -r requirements.txt\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f0db9c0-9205-4feb-875a-bb5148b1697f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "source_catalog_name = config[\"source_catalog_name\"]\n",
    "source_schema_name = config[\"source_schema_name\"]\n",
    "target_catalog_name = config[\"target_catalog_name\"]\n",
    "target_schema_name = config[\"target_schema_name\"]\n",
    "service_date = config[\"service_date\"]\n",
    "claim_id = config[\"claim_id\"]\n",
    "patient_id = config[\"patient_id\"]\n",
    "diagnosis_code = config[\"diagnosis_code\"]\n",
    "ndc_code = config[\"ndc_code\"]\n",
    "model_uc_name = config[\"model_uc_name\"]\n",
    "alias = config[\"alias\"]\n",
    "endpoint_name = config[\"endpoint_name\"]\n",
    "experiment_id = config['experiment_id']\n",
    "label_users = config[\"label_users\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47030fc2-1a19-4e9c-a8d8-5c7dfc5daa43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow \n",
    "mlflow.set_experiment(experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3b2379f-6cab-4576-bef8-bb07cced43a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "client.get_model_version_by_alias(model_uc_name, alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51dfad4b-b2dd-43f3-a1d8-a8e2b731d1cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_version_uri = f\"models:/{model_uc_name}@{alias}\"\n",
    "model = mlflow.pyfunc.load_model(model_version_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d5785a2-334c-43bd-a80e-e536e5e11ab8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model.predict({\"messages\": [{\"role\": \"user\", \"content\": f\"What enrollment information do you have for patient {patient_id}?\"}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d0d4b57-3eda-47c5-b8fe-abf0ad5ad824",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate the agent with [Agent Evaluation](https://docs.databricks.com/generative-ai/agent-evaluation/index.html)\n",
    "\n",
    "You can edit the requests or expected responses in your evaluation dataset and run evaluation as you iterate your agent, leveraging mlflow to track the computed quality metrics.\n",
    "\n",
    "To evaluate your tool calls, try adding [custom metrics](https://docs.databricks.com/generative-ai/agent-evaluation/custom-metrics.html#evaluating-tool-calls)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f46177ff-7b6d-42ab-a751-9e07a41e55db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Note: We will test with the specified patient_id to reduce the amount of data we are processing for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c6cf6a0-cfe4-4dc6-b47c-dbe53696d22e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "docs = spark.sql(\n",
    "    f\"\"\"\n",
    "    SELECT \n",
    "        patient_id as id, \n",
    "        CONCAT_WS(\n",
    "            ' ', \n",
    "            'PATIENT_ID:', patient_id, \n",
    "            'CLAIM_ID:', claim_id, \n",
    "            'DATE_SERVICE:', CAST(date_service AS STRING), \n",
    "            'LOCATION_OF_CARE:', location_of_care, \n",
    "            'PAY_TYPE:', pay_type\n",
    "        ) as content, \n",
    "        CONCAT_WS(' ', 'PATIENT_ID:', patient_id) as doc_uri\n",
    "    FROM ws_healthverity_patient_dataset.hv_claims_sample.medical_claim\n",
    "    WHERE patient_id = '{patient_id}'\n",
    "    ORDER BY date_service DESC\n",
    "    LIMIT 1000\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "display(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e512526-254d-49b6-865e-4db30780fc5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.agents.evals import generate_evals_df\n",
    "\n",
    "eval_data = generate_evals_df(\n",
    "  docs, \n",
    "  num_evals=10,   \n",
    "  agent_description=\"An agent that retrieves and analyzes patient healthcare data from HealthVerity's real-world healthcare dataset. The agent can use the following tools: get_patient_enrollment to get patient demographics and enrollment information, get_medical_claims to get medical claims for a patient on a specific service date, get_patient_diagnoses to get all diagnosis codes for a patient, get_pharmacy_claims to get pharmacy claims and medication history, and get_patient_procedures to get procedure codes and details for a patient.\",\n",
    "  question_guidelines=f\"\"\"\n",
    "  The agent can use the following tools:\n",
    "  - get_patient_enrollment: Takes a patient_id as input. Returns patient demographics, enrollment dates, and benefit details for a particular patient.\n",
    "  - get_medical_claims: Takes a patient_id and service_date as input. Returns medical claims information including claim id, service dates, location of care, and payment type for a particular patient on a specific service date.\n",
    "  - get_patient_diagnoses: Takes a patient_id as input. Returns diagnosis information including diagnosis codes, qualifiers, and service dates for a specific patient.\n",
    "  - get_pharmacy_claims: Takes a patient_id as input. Returns pharmacy claims information including NDC codes, fill information, days supply, and payment details for a given patient.\n",
    "  - get_patient_procedures: Takes a patient_id as input. Returns procedure information including procedure codes, service dates, units, and charges for a given patient.\n",
    "\n",
    "  Any dates that are returned should be in full date format when possible. \n",
    "\n",
    "  Focus on extracting comprehensive healthcare information. Use the provided tools to gather detailed data about the patient's healthcare journey including medical claims, pharmacy history, diagnoses, and procedures. You must include the relevant input in the prompt so that the LLM may use the relevant tool appropriately. For example, if you ask a question about a patient, include the patient's id in the prompt: What is the healthcare journey for patient '{patient_id}'? \n",
    "  \n",
    "  \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "772f1648-4d9d-4e6e-8a4c-f1bb4e2efcaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Let's take a look at the evals that we generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56b95600-28dc-4584-acfe-861b2d184b7b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"request_id\":204,\"inputs\":170,\"expectations\":549},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753903924675}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66c45d73-7efa-4c34-88d7-84f00551b36e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(eval_data).write.mode(\"append\").saveAsTable(f\"{target_catalog_name}.{target_schema_name}.patient_journey_assistant_eval_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ceac579-2afc-4824-80b7-61a2d849465a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## We are now going to use mlflow 3.x for the newest features!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3bd7fea-121c-468d-885a-9f8848eef700",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Wrapper function for predictions when evaluating"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.scorers import scorer\n",
    "\n",
    "# Wrap our model in predict function to handle parameter mapping \n",
    "@mlflow.trace\n",
    "def evaluate_model(messages: dict) -> dict:\n",
    "    return model.predict({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "146b0d3e-ee2f-4b59-91e0-ca52b3774d42",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define the guidelines for our evaluation"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.agents.evals import judges, metric\n",
    "from mlflow.genai.scorers import (\n",
    "    Correctness, RetrievalSufficiency,  \n",
    "    RelevanceToQuery, Safety, RetrievalGroundedness, RetrievalRelevance, Guidelines\n",
    ")\n",
    "\n",
    "# Define guidelines for scorer\n",
    "guidelines = {\n",
    "    \"clarity\": [\"Response must be clear and concise\"],\n",
    "    # supports str or list[str]\n",
    "    \"accuracy\": \"Response must be factually accurate\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d8c90fa-d134-434c-b968-9de5cded97f8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define a custom scorer for our evaluations"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.entities import Trace, Feedback\n",
    "from mlflow.genai.judges import is_context_relevant\n",
    "from mlflow.genai.scorers import scorer\n",
    "from typing import Any\n",
    "\n",
    "@scorer\n",
    "def is_message_relevant(inputs: dict[str, Any], outputs: str) -> Feedback:\n",
    "    # The `inputs` field for `sample_app` is a dictionary like: {\"messages\": [{\"role\": ..., \"content\": ...}, ...]}\n",
    "    # We need to extract the content of the last user message to pass to the relevance judge.\n",
    "\n",
    "    last_user_message_content = None\n",
    "    if \"messages\" in inputs and isinstance(inputs[\"messages\"], list):\n",
    "        for message in reversed(inputs[\"messages\"]):\n",
    "            if message.get(\"role\") == \"user\" and \"content\" in message:\n",
    "                last_user_message_content = message[\"content\"]\n",
    "                break\n",
    "\n",
    "    if not last_user_message_content:\n",
    "        raise Exception(\"Could not extract the last user message from inputs to evaluate relevance.\")\n",
    "\n",
    "    # Call the `relevance_to_query judge. It will return a Feedback object.\n",
    "    return is_context_relevant(\n",
    "        request=last_user_message_content,\n",
    "        context={\"response\": outputs},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c239f0c8-3bde-4d4c-8295-5d703f181808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Test Individual Judges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b776481e-11e2-4f4b-b3ef-1962fe55761f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_data[\"inputs\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e757efb-62a0-4db2-85f2-d804c90afa44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai import judges\n",
    "\n",
    "result = judges.is_correct(\n",
    "    request=eval_data[\"inputs\"][0],\n",
    "    response=model.predict(eval_data[\"inputs\"][0]),\n",
    "    expected_facts=eval_data[\"expectations\"][0][\"expected_facts\"]\n",
    ")\n",
    "print(f\"Judge result: {result.value}\")\n",
    "print(f\"Rationale: {result.rationale}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1803460-7faf-4295-8072-1a3ab4227626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = mlflow.genai.evaluate(\n",
    "    data=eval_data,\n",
    "    predict_fn=evaluate_model,\n",
    "    scorers=[\n",
    "        # With ground truth judges\n",
    "        Correctness(),\n",
    "        RetrievalSufficiency(),\n",
    "        Guidelines(name=\"clarity\", guidelines=guidelines[\"clarity\"]),\n",
    "        Guidelines(name=\"accuracy\", guidelines=guidelines[\"accuracy\"]),\n",
    "        # Without ground truth judges\n",
    "        RelevanceToQuery(),\n",
    "        Safety(),\n",
    "        RetrievalGroundedness(),\n",
    "        RetrievalRelevance(),\n",
    "        # Custom scorers\n",
    "        # check_no_pii,\n",
    "        is_message_relevant\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae9bd544-d523-4a32-909a-1654f1ef4606",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "traces = mlflow.search_traces(\n",
    "    run_id=results.run_id,\n",
    ")\n",
    "\n",
    "# Note: Modify the traces dictionary for proper retrieval (traces['trace'][0].info.assessments[0].rationale)\n",
    "for trace in traces['trace']:\n",
    "    assessments = trace.info.assessments\n",
    "    for assessment in assessments:\n",
    "        print(f\"Scorer: {assessment.name}\")\n",
    "        print(f\"Value: {assessment.value}\")\n",
    "        print(f\"Rationale: {assessment.rationale}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5c8d10b-a04f-43fb-94b7-72fdd1f02ec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Human Feedback / Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc3599ec-58a9-42be-b430-4f29210b9fe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- The Review App functionality has moved from databricks.agents to mlflow.genai.labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb26ab8c-9352-48a4-87d1-6c02e10ba3c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {target_catalog_name}.{target_schema_name}.augmented_eval_data (\n",
    "    dataset_record_id STRING,\n",
    "    create_time TIMESTAMP,\n",
    "    created_by STRING,\n",
    "    last_update_time TIMESTAMP,\n",
    "    last_updated_by STRING,\n",
    "    source STRUCT<\n",
    "        human: STRUCT<user_name: STRING>,\n",
    "        document: STRUCT<doc_uri: STRING, content: STRING>,\n",
    "        trace: STRUCT<trace_id: STRING>\n",
    "    >,\n",
    "    inputs STRING,\n",
    "    expectations STRING,\n",
    "    tags MAP<STRING, STRING>\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e55fc16-ac57-43fa-b158-6e36caeed2e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Eval Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "094c5de1-1dc4-4627-99f4-8844c6b53192",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.genai.datasets\n",
    "import time\n",
    "\n",
    "uc_schema = f\"{target_catalog_name}.{target_schema_name}\"\n",
    "# This table will be created in the above UC schema\n",
    "evaluation_dataset_table_name = \"full_evaluation_dataset\"\n",
    "\n",
    "try:\n",
    "    mlflow.genai.datasets.delete_dataset(\n",
    "        uc_table_name=f\"{uc_schema}.{evaluation_dataset_table_name}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Caught error: {e}\")\n",
    "\n",
    "try:\n",
    "    eval_dataset = mlflow.genai.datasets.create_dataset(\n",
    "        uc_table_name=f\"{uc_schema}.{evaluation_dataset_table_name}\",\n",
    "    )\n",
    "    print(f\"Created evaluation dataset: {uc_schema}.{evaluation_dataset_table_name}\")\n",
    "except Exception as e: \n",
    "    if \"TABLE_ALREADY_EXISTS\" in str(e):\n",
    "        print(\"Table already exists. Loading dataset instead.\")\n",
    "        eval_dataset = mlflow.genai.datasets.get_dataset(f\"{uc_schema}.{evaluation_dataset_table_name}\")\n",
    "        print(f\"Loaded evaluation dataset: {uc_schema}.{evaluation_dataset_table_name}\")\n",
    "    else:\n",
    "        print(f\"Caught error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5fd2243-0c62-40bc-a766-e5d96c9fb6d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "traces = mlflow.search_traces(\n",
    "    filter_string=\"attributes.status = 'OK'\",\n",
    "    order_by=[\"attributes.timestamp_ms DESC\"],\n",
    "    max_results=5\n",
    ")\n",
    "\n",
    "print(f\"Found {len(traces)} successful traces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "483968b2-b657-4740-be4f-2bd83dd32035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_dataset.merge_records(traces)\n",
    "print(f\"Added {len(traces)} records to evaluation dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0155d92-bd27-4452-834e-4ef6513d8927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preview the dataset\n",
    "eval_df = eval_dataset.to_df()\n",
    "print(f\"\\nDataset preview:\")\n",
    "print(f\"Total records: {len(eval_df)}\")\n",
    "print(\"\\nSample record:\")\n",
    "sample = eval_df.iloc[0]\n",
    "print(f\"Inputs: {sample['inputs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8012f9a3-eaa8-45e5-aaa8-97f040cc1990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Schema and Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "421208a0-81ab-4e6a-8763-18df96372c2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.genai.labeling as labeling\n",
    "import mlflow.genai.label_schemas as schemas\n",
    "\n",
    "quality_schema = schemas.create_label_schema(\n",
    "    name=\"response_quality\",\n",
    "    type=schemas.LabelSchemaType.FEEDBACK,\n",
    "    title=\"Rate the response quality\",\n",
    "    input=schemas.InputCategorical(\n",
    "        options=[\"Poor\", \"Fair\", \"Good\", \"Excellent\"]\n",
    "    ),\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "expected_facts_schema = schemas.create_label_schema(\n",
    "    name=schemas.EXPECTED_FACTS,\n",
    "    type=schemas.LabelSchemaType.EXPECTATION,\n",
    "    title=\"Expected facts\",\n",
    "    input=schemas.InputTextList(max_length_each=1000),\n",
    "    instruction=\"Please provide a list of facts that you expect to see in a correct response.\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Create labeling session\n",
    "session = labeling.create_labeling_session(\n",
    "    name=\"labeled_patient_journey_assistant_review_oct_2025\",\n",
    "    assigned_users=label_users,\n",
    "    label_schemas=[\n",
    "        schemas.EXPECTED_FACTS,\n",
    "        \"response_quality\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Created session: {session.name}\")\n",
    "print(f\"Session ID: {session.labeling_session_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2f8009ef-bf23-460d-9204-b51ce7ac7573",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Utility to delete sessions if needed"
    }
   },
   "outputs": [],
   "source": [
    "# import mlflow.genai.labeling as labeling\n",
    "\n",
    "# # Find the session to delete by name\n",
    "# all_sessions = labeling.get_labeling_sessions()\n",
    "# session_to_delete = None\n",
    "# for session in all_sessions:\n",
    "#     if session.name == \"labeled_patient_journey_assistant_review_oct_2025\":\n",
    "#         session_to_delete = session\n",
    "#         break\n",
    "\n",
    "# if session_to_delete:\n",
    "#     # Delete the session (removes from Review App)\n",
    "#     review_app = labeling.delete_labeling_session(session_to_delete)\n",
    "#     print(f\"Deleted session: {session_to_delete.name}\")\n",
    "# else:\n",
    "#     print(\"Session not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8aa315d-943c-4293-9045-0111cdcfa242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add traces for labeling\n",
    "traces = mlflow.search_traces(\n",
    "    run_id=session.mlflow_run_id\n",
    ")\n",
    "session.add_traces(traces)\n",
    "session.add_dataset(f\"{target_catalog_name}.{target_schema_name}.{evaluation_dataset_table_name}\")\n",
    "\n",
    "# Get review app URL\n",
    "app = labeling.get_review_app()\n",
    "\n",
    "# We need to explicitly add the agent to use \n",
    "if app.agents == None:\n",
    "    app.add_agent(\n",
    "        agent_name = \"patient_journey_assistant_agent\", \n",
    "        model_serving_endpoint= endpoint_name, \n",
    "        overwrite=False\n",
    "    )\n",
    "\n",
    "print(f\"Review app URL: {app.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c2c454c-98cb-4407-9a93-5ff31a35ebb9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sync the eval_data and evaluate"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai import datasets\n",
    "import mlflow\n",
    "\n",
    "# Sync expectations back to dataset\n",
    "session.sync(to_dataset=f\"{target_catalog_name}.{target_schema_name}.{evaluation_dataset_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7bfecf8-a7a5-4746-986f-c16cb2b3db13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use dataset for evaluation\n",
    "dataset = datasets.get_dataset(f\"{target_catalog_name}.{target_schema_name}.{evaluation_dataset_table_name}\")\n",
    "\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=dataset,\n",
    "    predict_fn=evaluate_model,\n",
    "    scorers=[\n",
    "        # With ground truth judges\n",
    "        Correctness(),\n",
    "        RetrievalSufficiency(),\n",
    "        Guidelines(name=\"clarity\", guidelines=guidelines[\"clarity\"]),\n",
    "        Guidelines(name=\"accuracy\", guidelines=guidelines[\"accuracy\"]),\n",
    "        # Without ground truth judges\n",
    "        RelevanceToQuery(),\n",
    "        Safety(),\n",
    "        RetrievalGroundedness(),\n",
    "        RetrievalRelevance(),\n",
    "        # Custom scorer\n",
    "        # check_no_pii,\n",
    "        is_message_relevant\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6446718906607140,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "03_agent_evaluations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
